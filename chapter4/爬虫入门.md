1. 什么是爬虫

简而言之，爬虫就是一段能够获取互联网信息（数据）的程序/工具。

一般需要通过抓取网页来获取互联网的信息与数据。

网页本身就是一个本文文件，只不过这个文本文件是由特定规则和符号标记的(HTML,超文本标记语言)，称为超文本文件，也可称为网页源代码。这段文本经过浏览器的解析（各类图片视频等在此过程中从网页外部加载），就成为我们日常浏览的网页展示给我们的样子了。

因此，爬虫首先就是要获得网页这个文本文件，并进行解析。如果所需数据就直接在网页文本文件中，则可直接得到；如果所需数据在网页外部，则通过解析的结果得到数据所在地址，将该数据下载。

2. 最简爬虫
 
这里我们以优秀的python第三方库requests为例，该库是“唯一一个非转基因的Python HTTP库，人类可以安全享用”。其中HTTP(HyperText Transfer Protocol)是互联网上应用最为广泛的一种网络协议，超文本文件传输时，必须遵守这个协议。非转基因是作者幽默的说法，表明该库是原汁原味符合python设计理念，易用易于理解。

输入以下代码，并观察执行结果。

```python
# coding: utf-8
# 示例代码 D-1

import requests

r = requests.get('https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md')
with open('0.md', 'w') as f:
    f.write(r.text)
```

示例代码D-1中：
- `import requests`，首先导入`requests`库
- `r = requests.get('https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md')`，调用requests的get方法，向网址https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md 发送获取(get)请求，该方法返回一个应答(Response)对象r，其中包含该网页的所有信息。
- `print(r.text)`，打印对象r的text属性，即网页的HTML文本。

打开`https://github.com/liupengyuan/python_tutorial/blob/master/chapter1/0.md`网页，在网页正文区域点击右键，选择查看源代码，会发现代码示例D-1确实已经获取/抓取这个网页的文本文件。至此，一个最简爬虫已经完成。

3. 爬虫进阶

- 任务1：抓取网易首页(www.163.com) 上的全部新闻，并以将所有新闻标题及新闻内容存入到一个文本文件中。

首先我们打开网易首页，在页面内copy一个新闻标题到剪贴板，然后用鼠标右键点击页面，并选择`查看源代码`(如用chrome浏览器可选择`检查`，进入开发者模式)。，会看到一片很乱的代码(其实就是html标记的文本)，如果有一些html基础(推荐教程：http://www.w3school.com.cn/html/index.asp)， 看起来就不会那么乱了。

可以对示例代码D-1稍加修改，就能够抓取到这个页面(代码)，但我们需要对这个网页源代码进行解析，定位到我们希望的内容，获取我们希望得到的数据。这里将使用优秀的第三方网页解析包Beautiful Soap (ver4)，该包已经随Anaconda安装，名称为bs4，并辅以对抓取页面的仔细分析。

在刚打开的源代码页按`Ctrl+F`查找刚才copy在剪贴板上的新闻标题，找到这个新闻标题的位置。这行大致是这样：` <li class="cm_fb"><a href="http://news.163.com/xxxxxx.html">yyyyyyyyy</a></li>`， 其中`xxxxx`及`yyyyyyy`随新闻标题不同而不同。其中，`yyyyyyyy`是新闻标题，`http://news.163.com/xxxxxx.html`是新闻标题对应的网页链接，访问该网页链接，就能进入该新闻的内容页面，因此这两项是我们感兴趣并希望抓取的内容。

我们在源代码页继续查看各个新闻标题，可以发现各个新闻其实都在标签对`<li>`及`</li>`之间。

输入以下代码，并观察执行结果。

```python
# coding: utf-8
# 示例代码 D-2

import requests
from bs4 import BeautifulSoup

r = requests.get('http://www.163.com')
f = open('news_163.txt', 'w', encoding = 'utf-8')

soap = BeautifulSoup(r.text, 'html.parser')
for model in soup.find_all('li'):
    f.write(str(model.contents))
    
f.close()
```
示例代码D-2中：
- `from bs4 import BeautifulSoup`，从bs4包中导入BeautifulSoup模块
- `soap = BeautifulSoup(r.text, 'html.parser')`，对网页文本，利用python内置的`html.parser`解析器，将`r.text`初始化为`BeautifulSoup`对象`soap`。
- `soup.find_all('li')`，调用`BeautifulSoup`对象`soap`的`find_all()`方法，找到标签`li`之间的所有内容，并返回一个结果集合(`ResultSet`)，集合中每一个元素为一个`Tag`对象，与html中对tag定义相同，此处即利用`<li>`及`</li>`进行标记的所有对象。
- 对每一个Tag对象model，取出其内容(contents属性,会返回一个list，内容是两个tag之间的所有内容)，并转换为字符串写入文件。

打开news_163.txt文件，观察内容，并找到与网页源文件对应的新闻标题与链接处，进行比较可以发现，示例代码D-2确实将所有新闻标题及相应链接得到了，但同时也获取了很多不需要的数据。

由于我们是找到了`<li>`及`</li>`进行标记的所有对象，而经过观察可以发现，这样的对象非常多，并非只有新闻标题与链接符合这一模式。我们只能更细致地分析网页源代码。

我们重新在网页源文件中查找新闻标题，进行观察，可以发现，新闻标题及链接均在`<ul class="cm_ul_round">`及`</ul>`之间(指向一种名称为`cm_ul_round`的样式)，且源文件中，其他非新闻标题的内容，均不在这种规定了`class`的`<ul>`标签之间，OK，找到关键标签了。
输入以下代码，并观察执行结果（查看所保存文件内容）。

```python
# coding: utf-8
# 示例代码 D-3

import requests
from bs4 import BeautifulSoup

r = requests.get('http://www.163.com')
f = open('news_163.txt', 'w', encoding = 'utf-8')

soup = BeautifulSoup(r.text, 'html.parser')

for model in soup.find_all(attrs={"class":"cm_ul_round"}):
    model = model.find_all('a')
    for text in model:
        f.write('{}:{}\n'.format(text.get('href'),text.string))
        
f.close()
```
示例代码 D-3中：
- ` soup.find_all(attrs={"class":"cm_ul_round"})`，find_all()中有个attrs 参数，可以定义一个字典参数来搜索包含特殊属性的tag，这个特殊属性`class="cm_ul_round"`以词典`{"class":"cm_ul_round"}`的形式传给attrs，就可以得到所有在`<ul class="cm_ul_round">`及`</ul>`之间的对象集合了。
- `model.find_all('a')`，得到其包含在标签`<a>`之间的对象集合。
- `text.get('href')`，利用tag对象的get()方法得到text对象的链接。
- `text.string`，利用tag对象的string属性，得到text对象的文本。

我们得到了首页上每个新闻标题及链接URL，我们下一步就是要利用这些URL来抓取新闻页面，并从中提取出正文。整个过程与前面示例代码D-3处理的过程基本类似，关键在于，找到正文在新闻网页源代码中所处的标签。

输入以下代码，并观察执行结果（查看所保存文件内容）。
```python
# coding: utf-8
# 示例代码 D-4

import requests
from bs4 import BeautifulSoup

# 第一部分，得到首页面新闻URL及标题，并存入dict
r = requests.get('http://www.163.com')
soup = BeautifulSoup(r.text, 'html.parser')
url_titles = {}

for model in soup.find_all(attrs={"class":"cm_ul_round"}):
    model = model.find_all('a')
    for text in model:
        try:
            url_titles[text.get('href')]=text.string
        except KeyError:
            pass

# 第二部分，对URL及标题的dict遍历，抓取每个新闻页面，提取正文并保存
f = open('news_163.txt', 'w', encoding = 'utf-8')

for url, title in url_titles.items():
    try:
        r = requests.get(url)
    except:
        pass
    soup = BeautifulSoup(r.text, 'html.parser')
    
    f.write('title:{}\n'.format(title))
    for model in soup.find_all(attrs={"class":"post_text"}):
        model = model.find_all('p')
        for text in model:
            if text.string:
                f.write(text.string)
    f.write('\n')
    
f.close()
```

示例代码D-4中：
- `url_titles[text.get('href')]=text.string`，这里以URL为键，以新闻标题为值，存入字典
- `soup.find_all(attrs={"class":"post_text"})`，通过观察源代码（或利用谷歌浏览器的检查），发现正文在`"class="cm_ul_round"`属性的标签之间，因此提取其间所有内容
- `model.find_all('p')`，提取`model`中所有标签`<p>与</p>`之间的内容

经过以上几个步骤，我们成功地将`www.163.com`首页所有新闻标题及正文抓取并存储到文件中了。当然，由于我们并没有特别仔细分析页面，最终抓取的文件中，仍然含有一部分不太正确的地方，留给读者自行解决。

- 任务2：抓取有道词典查询词的双语例句

4. 正则表达式
5. scrapy爬虫框架
6. 扩展
